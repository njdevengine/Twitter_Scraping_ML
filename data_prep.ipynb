{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Twitter for Corporate Espionage\n",
    "## (Using Webscraping, Facial Recognition and ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![headline](headline.png \"Twitter Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE COLOR NUMBERS COLUMN\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import urllib.request\n",
    "from PIL import Image as img\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('twitter_followers_detailed.xlsx')\n",
    "print('got df')\n",
    "n = 0\n",
    "for imageName in list(df[\"profile_image_url\"]):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(imageName, '/ml/out.png')\n",
    "        colors = len(img.open(\"ml/out.png\",mode=\"r\").getcolors(maxcolors=9999999))\n",
    "        df.at[n,\"color_number\"] = colors    \n",
    "    except:\n",
    "        df.at[n,\"color_number\"] = 0.01\n",
    "    n+=1\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Face Detection Column\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"ml\")\n",
    "except:\n",
    "    print(\"folder already exists...\")\n",
    "    \n",
    "import face_recognition\n",
    "\n",
    "n = 0\n",
    "for i in list(df[\"profile_image_url\"]):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(i, r'ml/out.png')\n",
    "        detection = face_recognition.load_image_file(\"ml/out.png\")\n",
    "        if face_recognition.face_locations(detection):\n",
    "            df.at[n,\"face_detection\"] = 1\n",
    "        else:\n",
    "            df.at[n,\"face_detection\"] = 0\n",
    "    except:\n",
    "        df.at[n,\"face_detection\"] = 999\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of photos that did not connect and default photos\n",
    "df = df[df.face_detection !=999.0]\n",
    "df = df[df.color_number != 184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL Detect Column\n",
    "array = []\n",
    "for i in list(df[\"url\"]):\n",
    "    if str(i) != \"nan\":\n",
    "        array.append(1)\n",
    "    else:\n",
    "        array.append(0)\n",
    "df[\"url_detect\"] = array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![headline](profile.png \"profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output people labeled individuals to a csv/ then convert it to text for NLTK analysis\n",
    "final = df\n",
    "\n",
    "i_bios = final[final.label == \"Individual\"][['bio']]\n",
    "i_bios.to_csv('i_bios.csv')\n",
    "\n",
    "import csv\n",
    "csv_file = (r'i_bios.csv')\n",
    "txt_file = (r'i_bios.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords function takes a filename, \"grams\" an integer meaning\n",
    "#1-2- or 3 word n-grams and minimum count or appearances of the word or phrase\n",
    "\n",
    "def keywords(file,grams,count):\n",
    "    with open(str(file),'r',encoding=\"latin1\") as myfile:\n",
    "        my_string=myfile.read().replace('\\n', '')\n",
    "\n",
    "    string = ''.join(ch for ch in my_string if ch not in exclude)\n",
    "\n",
    "    tokens = word_tokenize(string)\n",
    "    text = nltk.Text(tokens)\n",
    "\n",
    "    #array is the tuple of ngrams, array2 is the count of appearances, array1 is joined tuples, array 1 & 2 can be zipped into a dataframe\n",
    "    array =[]\n",
    "    array2 =[]\n",
    "    bgs = nltk.ngrams(tokens,int(grams))\n",
    "    fdist = nltk.FreqDist(bgs)\n",
    "    for k,v in fdist.items():\n",
    "        if v > int(count):\n",
    "            array.append(k)\n",
    "            array2.append(v)\n",
    "\n",
    "    array1 = []\n",
    "    for i in range(len(array)):\n",
    "        x = ' '.join(map(str,array[i]))\n",
    "        array1.append(x)\n",
    "\n",
    "    df = pd.DataFrame({'phrase':array1,'count': array2}).sort_values(by=\"count\",ascending=False)\n",
    "    for i in list(df['phrase']):\n",
    "        whitelist.append(i.lower())\n",
    "    df.to_csv('output_'+str(grams)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our dependancies, create empty whitelist array\n",
    "#exclude includes ,$\"@_[*|%)#+-<~^/;`=!:'&?}>({]\\. (things to filter out)\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "whitelist = []\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 1grams, appearing more than 30 times,bigrams more than 20,\n",
    "#and trigrams more than 10 times from the individuals bios\n",
    "keywords('i_bios.txt',1,30)\n",
    "keywords('i_bios.txt',2,20)\n",
    "keywords('i_bios.txt',3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from the individuals keywords array\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "new_whitelist = []\n",
    "\n",
    "stop = list(set(stopwords.words('english')))\n",
    "additional_stop = [\"business\"]\n",
    "stop += additional_stop\n",
    "safewords = [\"I\"]\n",
    "stop = [i for i in stop if i not in safewords]\n",
    "\n",
    "for i in whitelist:\n",
    "    try:\n",
    "        if (i.split(\" \")[0] not in stop) & (i.split(\" \")[1] not in stop):\n",
    "            new_whitelist.append(i)\n",
    "    except:\n",
    "        if i not in stop:\n",
    "            new_whitelist.append(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dupes, filter non-alpha keywords and save it to individual_keywords variable\n",
    "new_whitelist = list(set(new_whitelist))\n",
    "\n",
    "for i in range(len(new_whitelist)):\n",
    "    try:\n",
    "        if new_whitelist[i].split(\" \")[0].isalpha() == False:\n",
    "            new_whitelist.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "individual_keywords = new_whitelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![headline](individual_keywords.png \"Individual Keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset our variables and do all the same for business keywords\n",
    "new_whitelist = []\n",
    "whitelist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_bios = final[final.label == \"Business\"][[\"bio\"]]\n",
    "b_bios.to_csv('b_bios.csv')\n",
    "csv_file = (r'b_bios.csv')\n",
    "txt_file = (r'b_bios.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords('b_bios.txt',1,30)\n",
    "keywords('b_bios.txt',2,20)\n",
    "keywords('b_bios.txt',3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(set(stopwords.words('english')))\n",
    "additional_stop = [\"business\",\"us\"]\n",
    "stop += additional_stop\n",
    "safewords = [\"I\"]\n",
    "stop = [i for i in stop if i not in safewords]\n",
    "\n",
    "for i in whitelist:\n",
    "    try:\n",
    "        if (i.split(\" \")[0] not in stop) & (i.split(\" \")[1] not in stop):\n",
    "            new_whitelist.append(i)\n",
    "    except:\n",
    "        if i not in stop:\n",
    "            new_whitelist.append(i)\n",
    "        pass\n",
    "    \n",
    "new_whitelist = list(set(new_whitelist))\n",
    "\n",
    "# filter non-alpha stuff\n",
    "for i in range(len(new_whitelist)):\n",
    "    if new_whitelist[i].split(\" \")[0].isalpha() == False:\n",
    "        new_whitelist.pop(i)\n",
    "        \n",
    "business_keywords = new_whitelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![headline](business_keywords.png \"Individual Keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create keyword counter columns for individual keywords, and business keywords\n",
    "i_nums = []\n",
    "for x in range(len(final)):\n",
    "    test = final[\"bio\"][x]\n",
    "    n = 0\n",
    "    try:\n",
    "        for i in individual_keywords:\n",
    "            if i in test:\n",
    "                n+=1\n",
    "        i_nums.append(n)\n",
    "    except:\n",
    "        i_nums.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_nums = []\n",
    "for x in range(len(final)):\n",
    "    test = final[\"bio\"][x]\n",
    "    n = 0\n",
    "    try:\n",
    "        for i in business_keywords:\n",
    "            if i in test:\n",
    "                n+=1\n",
    "        b_nums.append(n)\n",
    "    except:\n",
    "        b_nums.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"b_key_count\"] = b_nums\n",
    "final[\"i_key_count\"] = i_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[[\"bio\",\"b_key_count\",\"i_key_count\"]].sort_values(by=\"b_key_count\",ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[[\"bio\",\"b_key_count\",\"i_key_count\"]].sort_values(by=\"i_key_count\",ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('final_output.csv',encoding=\"UTF-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
